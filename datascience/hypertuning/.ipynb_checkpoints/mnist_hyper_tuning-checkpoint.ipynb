{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9c5360cc22ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import time\n",
    "from random import randrange\n",
    "from past.builtins import xrange\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "###################################1##########################################\n",
    "# TODO: Print the shape of the training data and testing data               #\n",
    "# Plot the previous 9 training data and title their class                   #\n",
    "# Count the number of data for each class in training data                  #\n",
    "#############################################################################\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "ar1,ar2 = np.unique(y_train, return_counts=True)\n",
    "print(ar1)\n",
    "print(ar2)\n",
    "\n",
    "fig, axar =plt.subplots(3,3)\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(0,9):\n",
    "    axar[int(i/3),i%3].set_title(str(y_train[i]))\n",
    "    axar[int(i/3),i%3].imshow(X_train[i])\n",
    "    \n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation\n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.4, random_state = 0)\n",
    "for train_idx, val_idx in sss.split(X_train, y_train):\n",
    "    X_train_2, X_val_2 = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_2, y_val_2 = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "print (X_train_2.shape)\n",
    "print (y_train_2.shape)\n",
    "print (X_val_2.shape)\n",
    "print (y_val_2.shape)\n",
    "\n",
    "ar3,ar4 = np.unique(y_train_2, return_counts=True)\n",
    "print(ar3)\n",
    "print(ar4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_re = X_train_2.reshape(X_train_2.shape[0], 28*28)\n",
    "X_val_re = X_val_2.reshape(X_val_2.shape[0], 28*28)\n",
    "X_test_re = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "y_train_re = np.zeros((y_train_2.shape[0], 10))\n",
    "y_val_re = np.zeros((y_val_2.shape[0], 10))\n",
    "y_test_re = np.zeros((y_test.shape[0], 10))\n",
    "\n",
    "\n",
    "for i,j in enumerate(y_train_2):\n",
    "    y_train_re[i][int(j)] = 1\n",
    "for i,j in enumerate(y_val_2):\n",
    "    y_val_re[i][int(j)] = 1\n",
    "for i,j in enumerate(y_test):\n",
    "    y_test_re[i][int(j)] = 1\n",
    "\n",
    "\n",
    "\n",
    "print (X_train_re.shape)\n",
    "print (X_val_re.shape )\n",
    "print (X_test_re.shape )\n",
    "print (X_val_re.shape)\n",
    "print (y_train_2.shape)\n",
    "print (y_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(lr = 1e-4, transferfn = 'relu', batch_size = 512, epochs = 40, optimizer = Adam, kernel_initializer = initializers.RandomUniform() ):\n",
    "    dimension_ = 10\n",
    "    model1 = models.Sequential()\n",
    "    model1.add(layers.Dense(160, activation = transferfn, input_shape = (28*28, ), kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(250, activation = transferfn, kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(100, activation = transferfn, kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(160, activation = transferfn, kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(10, activation = 'softmax'))\n",
    "    model1.summary()\n",
    "\n",
    "\n",
    "    model1.compile(\n",
    "                  optimizer=optimizer(lr = lr),\n",
    "#                   optimizer = 'adam', \n",
    "                  loss = 'categorical_crossentropy', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model1.fit(X_train_re, \n",
    "                        y_train_re, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (X_val_re, y_val_re))\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    results = model1.evaluate(X_test_re, y_test_re)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_arr = [ 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "transfer_fn_arr = ['elu', 'selu', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "batch_size_arr = [20, 30, 50, 100, 200]\n",
    "epochs_arr = [100, 200, 300, 400, 500]\n",
    "optimizer_arr = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]\n",
    "weight_initialization_arr = [initializers.RandomUniform()\n",
    "                            , initializers.lecun_uniform()\n",
    "                            , initializers.he_uniform()\n",
    "                            , initializers.glorot_uniform()\n",
    "                            , initializers.lecun_normal()\n",
    "                            , initializers.he_normal()\n",
    "                            , initializers.glorot_normal()]\n",
    "\n",
    "lr_results = dict()\n",
    "transfer_fn_results = dict()\n",
    "batch_size_results = dict()\n",
    "epochs_results = dict()\n",
    "optimizer_results = []\n",
    "weight_initialization_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(lr_arr):\n",
    "    if(i == 0) :\n",
    "        lr_results[str(j)] = hyper_tuning(lr = j, epochs = 5)\n",
    "        continue\n",
    "    lr_results[str(j)] = hyper_tuning(lr = j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transfer_fn_arr:\n",
    "    transfer_fn_results[i] = hyper_tuning(transferfn = i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in batch_size_arr:\n",
    "    batch_size_results[str(i)] = hyper_tuning(batch_size = i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epochs_arr:\n",
    "    epochs_results[str(i)] = hyper_tuning(epochs = i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in optimizer_arr:\n",
    "    optimizer_results.append(hyper_tuning(optimizer = i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in weight_initialization_arr:\n",
    "    weight_initialization_results.append(hyper_tuning(kernel_initializer = i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lr_results)\n",
    "\n",
    "\n",
    "x_axis = lr_arr\n",
    "y_axis = []\n",
    "for i in lr_arr:\n",
    "    y_axis.append(lr_results[str(i)][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('lr - acc')\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/lr-acc')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (transfer_fn_arr)\n",
    "# print (transfer_fn_results)\n",
    "# print (transfer_fn_results['elu'])\n",
    "\n",
    "x_axis = transfer_fn_arr\n",
    "y_axis = []\n",
    "for i in transfer_fn_arr:\n",
    "    y_axis.append(transfer_fn_results[i][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('transfer - acc')\n",
    "plt.xlabel('transfer')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/transfer-acc')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_results\n",
    "\n",
    "\n",
    "x_axis = batch_size_arr\n",
    "y_axis = []\n",
    "for i in batch_size_arr:\n",
    "    y_axis.append(batch_size_results[str(i)][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('batch_size - acc')\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/batch_size-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_results\n",
    "\n",
    "\n",
    "x_axis = epochs_arr\n",
    "y_axis = []\n",
    "for i in epochs_arr:\n",
    "    y_axis.append(epochs_results[str(i)][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('epochs - acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/epochs-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_arr = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]\n",
    "print(optimizer_results)\n",
    "\n",
    "\n",
    "x_axis = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "y_axis = []\n",
    "for i,j in enumerate(x_axis):\n",
    "    y_axis.append(optimizer_results[i][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('optimizer - acc')\n",
    "plt.xlabel('optimizer')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/optimizer-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initializers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea6fbd407761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m weight_initialization_arr = [initializers.RandomUniform()\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0;34m,\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlecun_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0;34m,\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhe_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0;34m,\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglorot_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0;34m,\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlecun_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initializers' is not defined"
     ]
    }
   ],
   "source": [
    "weight_initialization_arr = [initializers.RandomUniform()\n",
    "                            , initializers.lecun_uniform()\n",
    "                            , initializers.he_uniform()\n",
    "                            , initializers.glorot_uniform()\n",
    "                            , initializers.lecun_normal()\n",
    "                            , initializers.he_normal()\n",
    "                            , initializers.glorot_normal()]\n",
    "\n",
    "print(weight_initialization_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_axis = ['RandomUniform','lecun_uniform', 'he_uniform', 'glorot_uniform', 'lecun_normal', 'he_normal', 'glorot_normal']\n",
    "y_axis = []\n",
    "for i,j in enumerate(x_axis):\n",
    "    y_axis.append(weight_initialization_results[i][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('weight_initialization - acc')\n",
    "plt.xlabel('weight_initialization')\n",
    "plt.ylabel('acc')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./imgs/weight_initialization-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = hyper_tuning(lr=0.0001, batch_size=100, epochs=400, optimizer=Adam, transferfn='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.196838284504837, 0.978]\n"
     ]
    }
   ],
   "source": [
    "print (best_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
