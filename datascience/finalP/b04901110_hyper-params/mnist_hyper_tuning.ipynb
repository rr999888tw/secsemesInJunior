{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import time\n",
    "from random import randrange\n",
    "from past.builtins import xrange\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "###################################1##########################################\n",
    "# TODO: Print the shape of the training data and testing data               #\n",
    "# Plot the previous 9 training data and title their class                   #\n",
    "# Count the number of data for each class in training data                  #\n",
    "#############################################################################\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "ar1,ar2 = np.unique(y_train, return_counts=True)\n",
    "print(ar1)\n",
    "print(ar2)\n",
    "\n",
    "fig, axar =plt.subplots(3,3)\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(0,9):\n",
    "    axar[int(i/3),i%3].set_title(str(y_train[i]))\n",
    "    axar[int(i/3),i%3].imshow(X_train[i])\n",
    "    \n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 28, 28)\n",
      "(36000,)\n",
      "(24000, 28, 28)\n",
      "(24000,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[3554 4045 3575 3678 3505 3253 3551 3759 3511 3569]\n"
     ]
    }
   ],
   "source": [
    "# split train and validation\n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.4, random_state = 0)\n",
    "for train_idx, val_idx in sss.split(X_train, y_train):\n",
    "    X_train_2, X_val_2 = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_2, y_val_2 = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "print (X_train_2.shape)\n",
    "print (y_train_2.shape)\n",
    "print (X_val_2.shape)\n",
    "print (y_val_2.shape)\n",
    "\n",
    "ar3,ar4 = np.unique(y_train_2, return_counts=True)\n",
    "print(ar3)\n",
    "print(ar4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 784)\n",
      "(24000, 784)\n",
      "(10000, 784)\n",
      "(24000, 784)\n",
      "(36000,)\n",
      "(24000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_re = X_train_2.reshape(X_train_2.shape[0], 28*28)\n",
    "X_val_re = X_val_2.reshape(X_val_2.shape[0], 28*28)\n",
    "X_test_re = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "y_train_re = np.zeros((y_train_2.shape[0], 10))\n",
    "y_val_re = np.zeros((y_val_2.shape[0], 10))\n",
    "y_test_re = np.zeros((y_test.shape[0], 10))\n",
    "\n",
    "\n",
    "for i,j in enumerate(y_train_2):\n",
    "    y_train_re[i][int(j)] = 1\n",
    "for i,j in enumerate(y_val_2):\n",
    "    y_val_re[i][int(j)] = 1\n",
    "for i,j in enumerate(y_test):\n",
    "    y_test_re[i][int(j)] = 1\n",
    "\n",
    "\n",
    "\n",
    "print (X_train_re.shape)\n",
    "print (X_val_re.shape )\n",
    "print (X_test_re.shape )\n",
    "print (X_val_re.shape)\n",
    "print (y_train_2.shape)\n",
    "print (y_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(lr = 1e-4, transferfn = 'relu', batch_size = 512, epochs = 40, optimizer = Adam, kernel_initializer = initializers.RandomUniform() ):\n",
    "    dimension_ = 10\n",
    "    model1 = models.Sequential()\n",
    "    model1.add(layers.Dense(160, activation = transferfn, input_shape = (28*28, ), kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(250, activation = transferfn, kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(100, activation = transferfn, kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(160, activation = transferfn, kernel_initializer = kernel_initializer))\n",
    "    model1.add(layers.Dense(10, activation = 'softmax'))\n",
    "    model1.summary()\n",
    "\n",
    "\n",
    "    model1.compile(\n",
    "                  optimizer=optimizer(lr = lr),\n",
    "#                   optimizer = 'adam', \n",
    "                  loss = 'categorical_crossentropy', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model1.fit(X_train_re, \n",
    "                        y_train_re, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (X_val_re, y_val_re))\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    results = model1.evaluate(X_test_re, y_test_re)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_arr = [ 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "transfer_fn_arr = ['elu', 'selu', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "batch_size_arr = [20, 30, 50, 100, 200]\n",
    "epochs_arr = [100, 200, 300, 400, 500]\n",
    "optimizer_arr = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]\n",
    "weight_initialization_arr = [initializers.RandomUniform()\n",
    "                            , initializers.lecun_uniform()\n",
    "                            , initializers.he_uniform()\n",
    "                            , initializers.glorot_uniform()\n",
    "                            , initializers.lecun_normal()\n",
    "                            , initializers.he_normal()\n",
    "                            , initializers.glorot_normal()]\n",
    "\n",
    "lr_results = dict()\n",
    "transfer_fn_results = dict()\n",
    "batch_size_results = dict()\n",
    "epochs_results = dict()\n",
    "optimizer_results = []\n",
    "weight_initialization_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(lr_arr):\n",
    "    if(i == 0) :\n",
    "        lr_results[str(j)] = hyper_tuning(lr = j, epochs = 5)\n",
    "        continue\n",
    "    lr_results[str(j)] = hyper_tuning(lr = j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transfer_fn_arr:\n",
    "    transfer_fn_results[i] = hyper_tuning(transferfn = i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in batch_size_arr:\n",
    "    batch_size_results[str(i)] = hyper_tuning(batch_size = i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epochs_arr:\n",
    "    epochs_results[str(i)] = hyper_tuning(epochs = i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in optimizer_arr:\n",
    "    optimizer_results.append(hyper_tuning(optimizer = i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in weight_initialization_arr:\n",
    "    weight_initialization_results.append(hyper_tuning(kernel_initializer = i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lr_results)\n",
    "\n",
    "\n",
    "x_axis = lr_arr\n",
    "y_axis = []\n",
    "for i in lr_arr:\n",
    "    y_axis.append(lr_results[str(i)][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('lr - acc')\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/lr-acc')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (transfer_fn_arr)\n",
    "# print (transfer_fn_results)\n",
    "# print (transfer_fn_results['elu'])\n",
    "\n",
    "x_axis = transfer_fn_arr\n",
    "y_axis = []\n",
    "for i in transfer_fn_arr:\n",
    "    y_axis.append(transfer_fn_results[i][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('transfer - acc')\n",
    "plt.xlabel('transfer')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/transfer-acc')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_results\n",
    "\n",
    "\n",
    "x_axis = batch_size_arr\n",
    "y_axis = []\n",
    "for i in batch_size_arr:\n",
    "    y_axis.append(batch_size_results[str(i)][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('batch_size - acc')\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/batch_size-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_results\n",
    "\n",
    "\n",
    "x_axis = epochs_arr\n",
    "y_axis = []\n",
    "for i in epochs_arr:\n",
    "    y_axis.append(epochs_results[str(i)][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('epochs - acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/epochs-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_arr = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]\n",
    "print(optimizer_results)\n",
    "\n",
    "\n",
    "x_axis = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "y_axis = []\n",
    "for i,j in enumerate(x_axis):\n",
    "    y_axis.append(optimizer_results[i][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('optimizer - acc')\n",
    "plt.xlabel('optimizer')\n",
    "plt.ylabel('acc')\n",
    "plt.savefig('./imgs/optimizer-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_initialization_arr = [initializers.RandomUniform()\n",
    "                            , initializers.lecun_uniform()\n",
    "                            , initializers.he_uniform()\n",
    "                            , initializers.glorot_uniform()\n",
    "                            , initializers.lecun_normal()\n",
    "                            , initializers.he_normal()\n",
    "                            , initializers.glorot_normal()]\n",
    "\n",
    "print(weight_initialization_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_axis = ['RandomUniform','lecun_uniform', 'he_uniform', 'glorot_uniform', 'lecun_normal', 'he_normal', 'glorot_normal']\n",
    "y_axis = []\n",
    "for i,j in enumerate(x_axis):\n",
    "    y_axis.append(weight_initialization_results[i][1])\n",
    "\n",
    "# print(x_axis)\n",
    "# print(y_axis)\n",
    "plt.plot(x_axis, y_axis, 'bo')\n",
    "# plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('weight_initialization - acc')\n",
    "plt.xlabel('weight_initialization')\n",
    "plt.ylabel('acc')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./imgs/weight_initialization-acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 160)               125600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               40250     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 160)               16160     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1610      \n",
      "=================================================================\n",
      "Total params: 208,720\n",
      "Trainable params: 208,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 36000 samples, validate on 24000 samples\n",
      "Epoch 1/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.4958 - acc: 0.8521 - val_loss: 0.2203 - val_acc: 0.9343\n",
      "Epoch 2/400\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 0.1703 - acc: 0.9493 - val_loss: 0.1734 - val_acc: 0.9482\n",
      "Epoch 3/400\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 0.1159 - acc: 0.9656 - val_loss: 0.1518 - val_acc: 0.9538\n",
      "Epoch 4/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.0855 - acc: 0.9742 - val_loss: 0.1265 - val_acc: 0.9620\n",
      "Epoch 5/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 0.0636 - acc: 0.9810 - val_loss: 0.1254 - val_acc: 0.9630\n",
      "Epoch 6/400\n",
      "36000/36000 [==============================] - 5s 126us/step - loss: 0.0484 - acc: 0.9849 - val_loss: 0.1191 - val_acc: 0.9658\n",
      "Epoch 7/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 0.0375 - acc: 0.9893 - val_loss: 0.1138 - val_acc: 0.9674\n",
      "Epoch 8/400\n",
      "36000/36000 [==============================] - 4s 121us/step - loss: 0.0267 - acc: 0.9926 - val_loss: 0.1141 - val_acc: 0.9689\n",
      "Epoch 9/400\n",
      "36000/36000 [==============================] - 5s 137us/step - loss: 0.0206 - acc: 0.9944 - val_loss: 0.1119 - val_acc: 0.9702\n",
      "Epoch 10/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 0.0167 - acc: 0.9955 - val_loss: 0.1157 - val_acc: 0.9700\n",
      "Epoch 11/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 0.0129 - acc: 0.9966 - val_loss: 0.1214 - val_acc: 0.9698\n",
      "Epoch 12/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 0.0098 - acc: 0.9971 - val_loss: 0.1186 - val_acc: 0.9714\n",
      "Epoch 13/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.1201 - val_acc: 0.9719\n",
      "Epoch 14/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.1291 - val_acc: 0.9703\n",
      "Epoch 15/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.1357 - val_acc: 0.9687\n",
      "Epoch 16/400\n",
      "36000/36000 [==============================] - 5s 130us/step - loss: 0.0133 - acc: 0.9956 - val_loss: 0.1395 - val_acc: 0.9682\n",
      "Epoch 17/400\n",
      "36000/36000 [==============================] - 3s 91us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.1310 - val_acc: 0.9712\n",
      "Epoch 18/400\n",
      "36000/36000 [==============================] - 3s 89us/step - loss: 0.0099 - acc: 0.9971 - val_loss: 0.1358 - val_acc: 0.9715\n",
      "Epoch 19/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1411 - val_acc: 0.9715\n",
      "Epoch 20/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.1320 - val_acc: 0.9732\n",
      "Epoch 21/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 0.0085 - acc: 0.9974 - val_loss: 0.1428 - val_acc: 0.9707\n",
      "Epoch 22/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.1415 - val_acc: 0.9724\n",
      "Epoch 23/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 0.0093 - acc: 0.9973 - val_loss: 0.1797 - val_acc: 0.9658\n",
      "Epoch 24/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.1512 - val_acc: 0.9693\n",
      "Epoch 25/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1324 - val_acc: 0.9740\n",
      "Epoch 26/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 4.7326e-04 - acc: 0.9999 - val_loss: 0.1318 - val_acc: 0.9753\n",
      "Epoch 27/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.1794 - val_acc: 0.9664\n",
      "Epoch 28/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.1438 - val_acc: 0.9726\n",
      "Epoch 29/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1377 - val_acc: 0.9750\n",
      "Epoch 30/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1519 - val_acc: 0.9738\n",
      "Epoch 31/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 0.0132 - acc: 0.9960 - val_loss: 0.1443 - val_acc: 0.9718\n",
      "Epoch 32/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1757 - val_acc: 0.9688\n",
      "Epoch 33/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1443 - val_acc: 0.9748\n",
      "Epoch 34/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1475 - val_acc: 0.9733\n",
      "Epoch 35/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.1464 - val_acc: 0.9725\n",
      "Epoch 36/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1389 - val_acc: 0.9753\n",
      "Epoch 37/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 2.2617e-04 - acc: 1.0000 - val_loss: 0.1345 - val_acc: 0.9760\n",
      "Epoch 38/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.0022e-04 - acc: 1.0000 - val_loss: 0.1350 - val_acc: 0.9765\n",
      "Epoch 39/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 7.2469e-05 - acc: 1.0000 - val_loss: 0.1361 - val_acc: 0.9769\n",
      "Epoch 40/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 5.7536e-05 - acc: 1.0000 - val_loss: 0.1370 - val_acc: 0.9768\n",
      "Epoch 41/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 4.6925e-05 - acc: 1.0000 - val_loss: 0.1381 - val_acc: 0.9771\n",
      "Epoch 42/400\n",
      "36000/36000 [==============================] - 4s 124us/step - loss: 3.8293e-05 - acc: 1.0000 - val_loss: 0.1393 - val_acc: 0.9770\n",
      "Epoch 43/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 3.1985e-05 - acc: 1.0000 - val_loss: 0.1403 - val_acc: 0.9774\n",
      "Epoch 44/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 2.6739e-05 - acc: 1.0000 - val_loss: 0.1416 - val_acc: 0.9775\n",
      "Epoch 45/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 2.2308e-05 - acc: 1.0000 - val_loss: 0.1428 - val_acc: 0.9776\n",
      "Epoch 46/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.8755e-05 - acc: 1.0000 - val_loss: 0.1440 - val_acc: 0.9774\n",
      "Epoch 47/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.5744e-05 - acc: 1.0000 - val_loss: 0.1452 - val_acc: 0.9774\n",
      "Epoch 48/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.3249e-05 - acc: 1.0000 - val_loss: 0.1462 - val_acc: 0.9775\n",
      "Epoch 49/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1110e-05 - acc: 1.0000 - val_loss: 0.1474 - val_acc: 0.9775\n",
      "Epoch 50/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 9.3481e-06 - acc: 1.0000 - val_loss: 0.1487 - val_acc: 0.9773\n",
      "Epoch 51/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 7.8860e-06 - acc: 1.0000 - val_loss: 0.1498 - val_acc: 0.9775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 6.6493e-06 - acc: 1.0000 - val_loss: 0.1510 - val_acc: 0.9776\n",
      "Epoch 53/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 5.6353e-06 - acc: 1.0000 - val_loss: 0.1526 - val_acc: 0.9775\n",
      "Epoch 54/400\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 4.7370e-06 - acc: 1.0000 - val_loss: 0.1535 - val_acc: 0.9775\n",
      "Epoch 55/400\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 3.9753e-06 - acc: 1.0000 - val_loss: 0.1547 - val_acc: 0.9772\n",
      "Epoch 56/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 3.3738e-06 - acc: 1.0000 - val_loss: 0.1559 - val_acc: 0.9775\n",
      "Epoch 57/400\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 2.8275e-06 - acc: 1.0000 - val_loss: 0.1573 - val_acc: 0.9772\n",
      "Epoch 58/400\n",
      "36000/36000 [==============================] - 3s 92us/step - loss: 2.4105e-06 - acc: 1.0000 - val_loss: 0.1584 - val_acc: 0.9774\n",
      "Epoch 59/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 2.0334e-06 - acc: 1.0000 - val_loss: 0.1599 - val_acc: 0.9774\n",
      "Epoch 60/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.7126e-06 - acc: 1.0000 - val_loss: 0.1609 - val_acc: 0.9775\n",
      "Epoch 61/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.4547e-06 - acc: 1.0000 - val_loss: 0.1619 - val_acc: 0.9776\n",
      "Epoch 62/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.2343e-06 - acc: 1.0000 - val_loss: 0.1635 - val_acc: 0.9773\n",
      "Epoch 63/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.0508e-06 - acc: 1.0000 - val_loss: 0.1644 - val_acc: 0.9775\n",
      "Epoch 64/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 8.9744e-07 - acc: 1.0000 - val_loss: 0.1656 - val_acc: 0.9775\n",
      "Epoch 65/400\n",
      "36000/36000 [==============================] - 3s 88us/step - loss: 7.6247e-07 - acc: 1.0000 - val_loss: 0.1673 - val_acc: 0.9773\n",
      "Epoch 66/400\n",
      "36000/36000 [==============================] - 3s 87us/step - loss: 6.6106e-07 - acc: 1.0000 - val_loss: 0.1682 - val_acc: 0.9775\n",
      "Epoch 67/400\n",
      "36000/36000 [==============================] - 3s 88us/step - loss: 5.6953e-07 - acc: 1.0000 - val_loss: 0.1695 - val_acc: 0.9776\n",
      "Epoch 68/400\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 4.9012e-07 - acc: 1.0000 - val_loss: 0.1707 - val_acc: 0.9773\n",
      "Epoch 69/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 4.3121e-07 - acc: 1.0000 - val_loss: 0.1719 - val_acc: 0.9773\n",
      "Epoch 70/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 3.7610e-07 - acc: 1.0000 - val_loss: 0.1731 - val_acc: 0.9774\n",
      "Epoch 71/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 3.3538e-07 - acc: 1.0000 - val_loss: 0.1742 - val_acc: 0.9776\n",
      "Epoch 72/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 2.9635e-07 - acc: 1.0000 - val_loss: 0.1755 - val_acc: 0.9775\n",
      "Epoch 73/400\n",
      "36000/36000 [==============================] - 3s 94us/step - loss: 2.6451e-07 - acc: 1.0000 - val_loss: 0.1768 - val_acc: 0.9775\n",
      "Epoch 74/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 2.3849e-07 - acc: 1.0000 - val_loss: 0.1779 - val_acc: 0.9774\n",
      "Epoch 75/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 2.1854e-07 - acc: 1.0000 - val_loss: 0.1791 - val_acc: 0.9776\n",
      "Epoch 76/400\n",
      "36000/36000 [==============================] - 3s 94us/step - loss: 2.0019e-07 - acc: 1.0000 - val_loss: 0.1805 - val_acc: 0.9775\n",
      "Epoch 77/400\n",
      "36000/36000 [==============================] - 3s 94us/step - loss: 1.8496e-07 - acc: 1.0000 - val_loss: 0.1822 - val_acc: 0.9772\n",
      "Epoch 78/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.7306e-07 - acc: 1.0000 - val_loss: 0.1829 - val_acc: 0.9773\n",
      "Epoch 79/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.6289e-07 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9773\n",
      "Epoch 80/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.5492e-07 - acc: 1.0000 - val_loss: 0.1856 - val_acc: 0.9772\n",
      "Epoch 81/400\n",
      "36000/36000 [==============================] - 4s 117us/step - loss: 1.4785e-07 - acc: 1.0000 - val_loss: 0.1867 - val_acc: 0.9771\n",
      "Epoch 82/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.4206e-07 - acc: 1.0000 - val_loss: 0.1879 - val_acc: 0.9772\n",
      "Epoch 83/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.3782e-07 - acc: 1.0000 - val_loss: 0.1888 - val_acc: 0.9773\n",
      "Epoch 84/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.3386e-07 - acc: 1.0000 - val_loss: 0.1900 - val_acc: 0.9773\n",
      "Epoch 85/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.3072e-07 - acc: 1.0000 - val_loss: 0.1908 - val_acc: 0.9774\n",
      "Epoch 86/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.2838e-07 - acc: 1.0000 - val_loss: 0.1917 - val_acc: 0.9772\n",
      "Epoch 87/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.2609e-07 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9773\n",
      "Epoch 88/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.2449e-07 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.9772\n",
      "Epoch 89/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.2302e-07 - acc: 1.0000 - val_loss: 0.1943 - val_acc: 0.9774\n",
      "Epoch 90/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.2216e-07 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 0.9773\n",
      "Epoch 91/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.2136e-07 - acc: 1.0000 - val_loss: 0.1957 - val_acc: 0.9774\n",
      "Epoch 92/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.2069e-07 - acc: 1.0000 - val_loss: 0.1969 - val_acc: 0.9773\n",
      "Epoch 93/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.2031e-07 - acc: 1.0000 - val_loss: 0.1970 - val_acc: 0.9774\n",
      "Epoch 94/400\n",
      "36000/36000 [==============================] - 4s 122us/step - loss: 1.1993e-07 - acc: 1.0000 - val_loss: 0.1978 - val_acc: 0.9773\n",
      "Epoch 95/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1968e-07 - acc: 1.0000 - val_loss: 0.1981 - val_acc: 0.9774\n",
      "Epoch 96/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1951e-07 - acc: 1.0000 - val_loss: 0.1984 - val_acc: 0.9772\n",
      "Epoch 97/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1939e-07 - acc: 1.0000 - val_loss: 0.1991 - val_acc: 0.9773\n",
      "Epoch 98/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1933e-07 - acc: 1.0000 - val_loss: 0.1991 - val_acc: 0.9772\n",
      "Epoch 99/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1928e-07 - acc: 1.0000 - val_loss: 0.1994 - val_acc: 0.9771\n",
      "Epoch 100/400\n",
      "36000/36000 [==============================] - 5s 127us/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 0.1999 - val_acc: 0.9773\n",
      "Epoch 101/400\n",
      "36000/36000 [==============================] - 4s 125us/step - loss: 1.1923e-07 - acc: 1.0000 - val_loss: 0.2003 - val_acc: 0.9771\n",
      "Epoch 102/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1922e-07 - acc: 1.0000 - val_loss: 0.2001 - val_acc: 0.9772\n",
      "Epoch 103/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2001 - val_acc: 0.9772\n",
      "Epoch 104/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2004 - val_acc: 0.9771\n",
      "Epoch 105/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2006 - val_acc: 0.9772\n",
      "Epoch 106/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.9773\n",
      "Epoch 107/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.9772\n",
      "Epoch 108/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9772\n",
      "Epoch 109/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2007 - val_acc: 0.9772\n",
      "Epoch 110/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2007 - val_acc: 0.9773\n",
      "Epoch 111/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2008 - val_acc: 0.9773\n",
      "Epoch 112/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2008 - val_acc: 0.9773\n",
      "Epoch 113/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2006 - val_acc: 0.9772\n",
      "Epoch 114/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 115/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2011 - val_acc: 0.9773\n",
      "Epoch 116/400\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9772\n",
      "Epoch 117/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9772\n",
      "Epoch 118/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9772\n",
      "Epoch 119/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9772\n",
      "Epoch 120/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9772\n",
      "Epoch 121/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9772\n",
      "Epoch 122/400\n",
      "36000/36000 [==============================] - 3s 94us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 123/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9773\n",
      "Epoch 124/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 125/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 126/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 127/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 128/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 129/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 130/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 131/400\n",
      "36000/36000 [==============================] - 5s 130us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 132/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 133/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 134/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 135/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 136/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 137/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 138/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 139/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 140/400\n",
      "36000/36000 [==============================] - 4s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 141/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 142/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 143/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 144/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 145/400\n",
      "36000/36000 [==============================] - 4s 122us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 146/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 147/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 148/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 149/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 150/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 151/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 152/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 153/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 154/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 155/400\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 156/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 157/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 158/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 159/400\n",
      "36000/36000 [==============================] - 4s 117us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 160/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 161/400\n",
      "36000/36000 [==============================] - 5s 137us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 162/400\n",
      "36000/36000 [==============================] - 4s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 163/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 164/400\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 165/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/400\n",
      "36000/36000 [==============================] - 3s 96us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 167/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 168/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 169/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 170/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 171/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 172/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 173/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 174/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 175/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 176/400\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 177/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 178/400\n",
      "36000/36000 [==============================] - 5s 132us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 179/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 180/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 181/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 182/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 183/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 184/400\n",
      "36000/36000 [==============================] - 5s 131us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 185/400\n",
      "36000/36000 [==============================] - 5s 131us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 186/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 187/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 188/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 189/400\n",
      "36000/36000 [==============================] - 3s 91us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 190/400\n",
      "36000/36000 [==============================] - 3s 89us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 191/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 192/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 193/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 194/400\n",
      "36000/36000 [==============================] - 5s 132us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 195/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 196/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 197/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 198/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 199/400\n",
      "36000/36000 [==============================] - 5s 137us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 200/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 201/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 202/400\n",
      "36000/36000 [==============================] - 4s 123us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 203/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 204/400\n",
      "36000/36000 [==============================] - 5s 137us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 205/400\n",
      "36000/36000 [==============================] - 7s 204us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 206/400\n",
      "36000/36000 [==============================] - 4s 123us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 207/400\n",
      "36000/36000 [==============================] - 5s 153us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 208/400\n",
      "36000/36000 [==============================] - 7s 191us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 209/400\n",
      "36000/36000 [==============================] - 6s 164us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 210/400\n",
      "36000/36000 [==============================] - 6s 154us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 211/400\n",
      "36000/36000 [==============================] - 5s 146us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 212/400\n",
      "36000/36000 [==============================] - 6s 168us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 213/400\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 214/400\n",
      "36000/36000 [==============================] - 6s 164us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 215/400\n",
      "36000/36000 [==============================] - 6s 170us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 216/400\n",
      "36000/36000 [==============================] - 6s 173us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 217/400\n",
      "36000/36000 [==============================] - 7s 203us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 218/400\n",
      "36000/36000 [==============================] - 7s 186us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 219/400\n",
      "36000/36000 [==============================] - 6s 178us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 220/400\n",
      "36000/36000 [==============================] - 5s 147us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 221/400\n",
      "36000/36000 [==============================] - 6s 167us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 222/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 223/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 224/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 225/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 226/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 227/400\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 228/400\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 229/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 230/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 231/400\n",
      "36000/36000 [==============================] - 5s 143us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 232/400\n",
      "36000/36000 [==============================] - 5s 125us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 233/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 234/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 235/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 236/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 237/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 238/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 239/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 240/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 241/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 242/400\n",
      "36000/36000 [==============================] - 5s 145us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 243/400\n",
      "36000/36000 [==============================] - 7s 186us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 244/400\n",
      "36000/36000 [==============================] - 6s 165us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 245/400\n",
      "36000/36000 [==============================] - 5s 131us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 246/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 247/400\n",
      "36000/36000 [==============================] - 7s 199us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 248/400\n",
      "36000/36000 [==============================] - 8s 209us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 249/400\n",
      "36000/36000 [==============================] - 6s 173us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 250/400\n",
      "36000/36000 [==============================] - 4s 123us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 251/400\n",
      "36000/36000 [==============================] - 4s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 252/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 253/400\n",
      "36000/36000 [==============================] - 5s 125us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 254/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 255/400\n",
      "36000/36000 [==============================] - 5s 134us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 256/400\n",
      "36000/36000 [==============================] - 5s 129us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 257/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 258/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 259/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 260/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 261/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 262/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 263/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 264/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 265/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 266/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 267/400\n",
      "36000/36000 [==============================] - 4s 123us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 268/400\n",
      "36000/36000 [==============================] - 4s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 269/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 270/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 271/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 272/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 273/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 274/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 275/400\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 276/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 277/400\n",
      "36000/36000 [==============================] - 5s 144us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 278/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 279/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 280/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 281/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 282/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 283/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 284/400\n",
      "36000/36000 [==============================] - 4s 125us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 285/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 286/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 287/400\n",
      "36000/36000 [==============================] - 5s 129us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 288/400\n",
      "36000/36000 [==============================] - 5s 143us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 289/400\n",
      "36000/36000 [==============================] - 4s 117us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 290/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 291/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 292/400\n",
      "36000/36000 [==============================] - 4s 117us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 293/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 294/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 295/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 296/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 297/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 298/400\n",
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 299/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 300/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 301/400\n",
      "36000/36000 [==============================] - 5s 126us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 302/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 303/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 304/400\n",
      "36000/36000 [==============================] - 5s 138us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 305/400\n",
      "36000/36000 [==============================] - 4s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 306/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 307/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 308/400\n",
      "36000/36000 [==============================] - 5s 138us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 309/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 310/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 311/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 312/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 313/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 314/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 315/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 316/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 317/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 318/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 319/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 320/400\n",
      "36000/36000 [==============================] - 5s 130us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 321/400\n",
      "36000/36000 [==============================] - 5s 135us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 322/400\n",
      "36000/36000 [==============================] - 4s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 323/400\n",
      "36000/36000 [==============================] - 4s 122us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 324/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 325/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 326/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 327/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 328/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 329/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 330/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 331/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 332/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 333/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 334/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 4s 115us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 335/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 336/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 337/400\n",
      "36000/36000 [==============================] - 5s 129us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 338/400\n",
      "36000/36000 [==============================] - 4s 98us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 339/400\n",
      "36000/36000 [==============================] - 4s 99us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 340/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 341/400\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 342/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 343/400\n",
      "36000/36000 [==============================] - 5s 137us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 344/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 345/400\n",
      "36000/36000 [==============================] - 4s 100us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 346/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 347/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 348/400\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 349/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 350/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 351/400\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 352/400\n",
      "36000/36000 [==============================] - 4s 125us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 353/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 354/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 355/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 356/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 357/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 358/400\n",
      "36000/36000 [==============================] - 4s 107us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 359/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 360/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 361/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 362/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 363/400\n",
      "36000/36000 [==============================] - 5s 127us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 364/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 365/400\n",
      "36000/36000 [==============================] - 5s 134us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 366/400\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 367/400\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 368/400\n",
      "36000/36000 [==============================] - 4s 102us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 369/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 370/400\n",
      "36000/36000 [==============================] - 4s 106us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 371/400\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 372/400\n",
      "36000/36000 [==============================] - 4s 105us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 373/400\n",
      "36000/36000 [==============================] - 4s 104us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 374/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 375/400\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 376/400\n",
      "36000/36000 [==============================] - 5s 149us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 377/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 378/400\n",
      "36000/36000 [==============================] - 4s 110us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 379/400\n",
      "36000/36000 [==============================] - 4s 112us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 380/400\n",
      "36000/36000 [==============================] - 4s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 381/400\n",
      "36000/36000 [==============================] - 5s 145us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 382/400\n",
      "36000/36000 [==============================] - 4s 113us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 383/400\n",
      "36000/36000 [==============================] - 5s 137us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 384/400\n",
      "36000/36000 [==============================] - 4s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 385/400\n",
      "36000/36000 [==============================] - 4s 114us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 386/400\n",
      "36000/36000 [==============================] - 5s 129us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 387/400\n",
      "36000/36000 [==============================] - 3s 97us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 388/400\n",
      "36000/36000 [==============================] - 4s 101us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 389/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 390/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 5s 128us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 391/400\n",
      "36000/36000 [==============================] - 4s 109us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 392/400\n",
      "36000/36000 [==============================] - 5s 126us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 393/400\n",
      "36000/36000 [==============================] - 4s 111us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 394/400\n",
      "36000/36000 [==============================] - 3s 95us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 395/400\n",
      "36000/36000 [==============================] - 3s 90us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 396/400\n",
      "36000/36000 [==============================] - 3s 91us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 397/400\n",
      "36000/36000 [==============================] - 4s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 398/400\n",
      "36000/36000 [==============================] - 5s 140us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 399/400\n",
      "36000/36000 [==============================] - 4s 108us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n",
      "Epoch 400/400\n",
      "36000/36000 [==============================] - 4s 103us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9773\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 51us/step\n"
     ]
    }
   ],
   "source": [
    "best_result = hyper_tuning(lr=0.0001, batch_size=100, epochs=400, optimizer=Adam, transferfn='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.196838284504837, 0.978]\n"
     ]
    }
   ],
   "source": [
    "print (best_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
